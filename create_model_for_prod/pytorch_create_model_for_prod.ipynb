{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import UtilsKy\n",
    "from analyzer import AnalyzerPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch mlp for binary classification\n",
    "from numpy import vstack\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_teach = pd.read_csv( UtilsKy.DB_TEACH_KYW3, dtype=str, encoding='cp1251')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_test = pd.read_csv(UtilsKy.DB_TEST_KYW3, dtype=str, encoding='cp1251')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "white = pd.read_csv(UtilsKy.WHITE_KYW3 , dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_FACTORS = ['amount', 'bank_currency', 'bin', 'count_months_to_end_card', 'day_of_week', 'is_city_resolved', 'hour',\n",
    "                                                             'is_gender_undefined', 'latitude', 'longitude', 'phone_2_norm']\n",
    "COL_FACTORS = sorted(COL_FACTORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train na columns : Index(['latitude', 'longitude'], dtype='object')\n",
      "test na columns : Index(['latitude', 'longitude'], dtype='object')\n",
      "-92.53325861542274\n",
      "-999\n",
      "-999\n",
      "-999\n",
      "36.90237577890762\n",
      "-999\n",
      "-999\n",
      "-999\n",
      "-999\n",
      "-999\n",
      "-999\n"
     ]
    }
   ],
   "source": [
    "from helper import DataHelper\n",
    "datahelper = DataHelper(db_teach, db_test, COL_FACTORS)\n",
    "datahelper.create_train_test()\n",
    "datahelper.show_columns_with_na()\n",
    "mean_values = datahelper.get_mean_value()\n",
    "replaced_values = { col: mean_values[col] for col in ('latitude', 'longitude')}\n",
    "replaced_values['default'] =  -999\n",
    "datahelper.replaced_na_values(replaced_values) \n",
    "scaler_params = datahelper.get_scaler_params()\n",
    "datahelper.minMaxScaler_own()\n",
    "datahelper.add_status_in_train_test()\n",
    "train , test = datahelper.get_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, df): # path\n",
    "        # store the inputs and outputs\n",
    "        self.X = df.values[:, :-1]\n",
    "        self.y = df.values[:, -1]\n",
    "        # ensure input data is floats\n",
    "        self.X = self.X.astype('float32')\n",
    "        # label encode target and ensure the values are floats\n",
    "        self.y = LabelEncoder().fit_transform(self.y)\n",
    "        self.y = self.y.astype('float32')\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    " \n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    " \n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 10)\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.act1 = ReLU()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(10, 6)\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(6, 1)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "        self.act3 = Sigmoid()\n",
    " \n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset\n",
    "def prepare_data(df):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(df)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dl, model, optimizer=None, n_epoch=100, lr=0.01, momentum=0.9):\n",
    "    criterion = BCELoss()\n",
    "    if optimizer == None:\n",
    "        optimizer = SGD(model.parameters(), lr=lr, momentum=momentum)   \n",
    "    for epoch in range(n_epoch):\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            optimizer.zero_grad()\n",
    "            yhat = model(inputs)\n",
    "            loss = criterion(yhat, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_test_prediction(db_test, model, numeric_cols):\n",
    "    predictions = list()\n",
    "    test = db_test[numeric_cols].values\n",
    "    for row in test:\n",
    "        predictions.append(predict(row, model)[0][0])\n",
    "    return predictions         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, _ = prepare_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(COL_FACTORS)\n",
    "model = MLP(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.MLP"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_prediction =  AnalyzerPrediction(db_teach, db_test, white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_amount = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epoch=30\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 30\n",
    "lr = 0.01 \n",
    "momentum = 0.9\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "train_model(train_dl, model, optimizer, n_epoch=n_epoch, lr=lr, momentum=momentum)\n",
    "\n",
    "test_probability = get_db_test_prediction(test, model, COL_FACTORS)\n",
    "db_test[\"probability\"] = test_probability\n",
    "\n",
    "description = '-' . join([str(elem) for elem in (n_epoch, momentum, lr)])       \n",
    "result_df_amount = analyzer_prediction.get_table_prediction(description=description, result_df=result_df_amount, metric=\"amount\")\n",
    "print( 'n_epoch={}'. format(n_epoch ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = result_df_amount.shape[0]\n",
    "sub_rows = list(range(n))[::2]\n",
    "stat_best = result_df_amount.copy().iloc[sub_rows,:]\n",
    "\n",
    "col_names = [col for col in stat_best.columns if col.startswith('p_') ] \n",
    "stat_best.loc[:, col_names] = stat_best.loc[:, col_names].astype(float)\n",
    "stat_best = stat_best.sort_values(by=\"rating\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>p_1</th>\n",
       "      <th>p_2</th>\n",
       "      <th>p_3</th>\n",
       "      <th>p_4</th>\n",
       "      <th>p_5</th>\n",
       "      <th>p_6</th>\n",
       "      <th>p_7</th>\n",
       "      <th>p_10</th>\n",
       "      <th>p_20</th>\n",
       "      <th>rating</th>\n",
       "      <th>n_white_list</th>\n",
       "      <th>n_test_in_wl</th>\n",
       "      <th>n_test_bad_in_wl</th>\n",
       "      <th>amount_test_in_wl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30-0.9-0.01</td>\n",
       "      <td>5.22</td>\n",
       "      <td>9.55</td>\n",
       "      <td>13.62</td>\n",
       "      <td>15.9</td>\n",
       "      <td>18.86</td>\n",
       "      <td>21.91</td>\n",
       "      <td>26.18</td>\n",
       "      <td>32.19</td>\n",
       "      <td>48.57</td>\n",
       "      <td>143.91</td>\n",
       "      <td>1019125</td>\n",
       "      <td>22992</td>\n",
       "      <td>34</td>\n",
       "      <td>1640236.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description   p_1   p_2    p_3   p_4    p_5    p_6    p_7   p_10   p_20  \\\n",
       "0  30-0.9-0.01  5.22  9.55  13.62  15.9  18.86  21.91  26.18  32.19  48.57   \n",
       "\n",
       "   rating  n_white_list  n_test_in_wl  n_test_bad_in_wl  amount_test_in_wl  \n",
       "0  143.91       1019125         22992                34         1640236.53  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_best.iloc[:,:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_model = {'profile': model, 'algorithm_name': 'pytorch', 'factor_list': COL_FACTORS, 'replaced_values': replaced_values, \n",
    "              'scaler_params': scaler_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-01-25'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import date\n",
    "today = date.today()\n",
    "today = today.strftime(\"%Y-%m-%d\")\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "file_name =  '_' . join([str(elem) for elem in ('pytorch', 'kyw3',  description, today)]) \n",
    "file_name  = re.sub(\"[.]\", \"\", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pytorch_kyw3_30_09_001_2021_01_25'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_class_name =re.sub(\"[-]\", \"_\", file_name)\n",
    "file_class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pytorch_kyw3_30-09-001_2021-01-25']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(conf_model, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to modlel class(MLP) For load model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
