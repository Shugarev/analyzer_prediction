{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "pd.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistic import Statistic\n",
    "from utils import UtilsKy\n",
    "from analyzer import HelperAnalyzer, AnalyzerPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kyw3\n",
    "#path_data = '/mnt/files/workdata/work/merchants/merchant_33_kyw3_2020-06-05/04_experiments/ex_01_some_teach/'\n",
    "db_teach = pd.read_csv(UtilsKy.DB_TEACH_KYW3, dtype=str, encoding='cp1251')\n",
    "db_test = pd.read_csv(UtilsKy.DB_TEST_KYW3, dtype=str, encoding='cp1251')\n",
    "white = pd.read_csv(UtilsKy.WHITE_KYW3 , dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ky9\n",
    "# path_data = '/mnt/files/workdata/work/merchants/merchant_32_ky9_2020-05-12_white_visa/04_experiments/'\n",
    "# db_teach = pd.read_csv(UtilsKy.DB_TEACH_KY9, dtype=str)\n",
    "# db_test = pd.read_csv(UtilsKy.DB_TEST_KY9, dtype=str)\n",
    "# white = pd.read_csv(UtilsKy.WHITE_KY9 , dtype=str)\n",
    "\n",
    "# for prod\n",
    "# db_teach = pd.read_csv(UtilsKy.DB_TEACH_KY9_FOR_PROD, dtype=str)\n",
    "# db_test = pd.read_csv(UtilsKy.DB_TEST_KY9_FOR_PROD , dtype=str)\n",
    "# white = pd.read_csv(UtilsKy.WHITE_KY9 , dtype=str)\n",
    "\n",
    "# db_teach = pd.read_csv(path_data + 'ex_05_ky9_xgb_jupiter_2020_07_08/db_teach_ky9_is_frequency_ip.csv', dtype=str)\n",
    "# db_test = pd.read_csv(path_data + 'ex_05_ky9_xgb_jupiter_2020_07_08/db_test_ky9_is_frequency_ip.csv', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['amount', 'amount_deviation', 'bank_currency', 'bin', 'city',\n",
       "       'count_months_to_end_card', 'day_of_week', 'gender2', 'hour', 'id',\n",
       "       'is_city_resolved', 'is_gender_undefined', 'latitude', 'longitude',\n",
       "       'order_id', 'phone_2_norm', 'status'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_teach.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    427164\n",
       "1      6261\n",
       "Name: status, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistic.get_table_value_counts(db_teach, 'status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    58107\n",
       "1      755\n",
       "Name: status, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistic.get_table_value_counts(db_test, 'status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_FACTORS = ['bin', 'amount', 'bank_currency', 'hour', 'day_of_week', 'longitude', 'latitude', 'phone_2_norm', 'is_gender_undefined', 'is_city_resolved']\n",
    "COL_FACTORS = sorted(COL_FACTORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train na columns : Index(['latitude', 'longitude'], dtype='object')\n",
      "test na columns : Index(['latitude', 'longitude'], dtype='object')\n",
      "36.90237577890762\n",
      "-999\n",
      "-999\n",
      "-999\n",
      "-999\n",
      "-999\n",
      "-999\n",
      "-999\n",
      "-92.53325861542274\n",
      "-999\n"
     ]
    }
   ],
   "source": [
    "# For Xgboost\n",
    "from helper import DataHelper\n",
    "datahelper = DataHelper(db_teach, db_test, COL_FACTORS)\n",
    "datahelper.create_train_test()\n",
    "datahelper.show_columns_with_na()\n",
    "mean_values = datahelper.get_mean_value()\n",
    "replaced_values = { col: mean_values[col] for col in ('latitude', 'longitude')}\n",
    "replaced_values['default'] =  -999\n",
    "datahelper.replaced_na_values(replaced_values)   \n",
    "train , test = datahelper.get_train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatenate teach set and test set for validation \n",
    "##### Using GridSerch() for Best params\n",
    "###### For any set of parameters we have two experiments(PredefinedSplit)\n",
    "###### 1. Train is sample from teach set and validate is sample from test set.\n",
    "###### 2. Train is sample from test set and validate is sample from teach set.\n",
    "##### For any experiment we calculate custom metric.\n",
    "##### We take mean custom metric from two experiment. \n",
    "##### Best parameters - are parameters with best metric result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train = pd.concat([train, test], ignore_index=True).copy()\n",
    "total_label = pd.concat([db_teach.status, db_test.status], ignore_index=True, axis=0).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 69\n",
    "total_weight = np.where(total_label =='0', 1, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.PredefinedSplit.html#sklearn.model_selection.PredefinedSplit\n",
    "from sklearn.model_selection import PredefinedSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_prediction =  AnalyzerPrediction(db_teach, db_test, white)\n",
    "\n",
    "def custom_amount_score_p(y_true, y_predicted):\n",
    "       \n",
    "    if len(y_true) == len(db_test):\n",
    "        db_test[\"probability\"] = y_predicted\n",
    "    else: \n",
    "        print('custom_amount_score_p')\n",
    "        return 0\n",
    "    \n",
    "    message = ''\n",
    "    percents_cumsum = 0\n",
    "    for sample_percent in (1, 2, 3, 4, 5, 6, 7, 10, 20):\n",
    "        percent_bad_sample = analyzer_prediction.get_amount_3ds(sample_percent)[0]\n",
    "        percents_cumsum += percent_bad_sample \n",
    "        message += 'p_{}={},' . format(sample_percent, percent_bad_sample )  \n",
    "        \n",
    "    percents_cumsum = round(percents_cumsum, 2)     \n",
    "    message += ' total score={}'.format(percents_cumsum)       \n",
    "    print(message)\n",
    "    \n",
    "    return 2 * percents_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html#scoring\n",
    "\n",
    "# For GreedsearchCV method\n",
    "n_test = test.shape[0]\n",
    "def custom_score_p(y_true, y_predicted):\n",
    "    ''' Percent bad in sample.'''\n",
    "    score = 0\n",
    "    n_bad = y_true.sum()\n",
    "    print('custom_scoring n_test={} ' . format(len(y_true)))\n",
    "    message = ''\n",
    "    for sample_percent in (1,2,3,4,5,6,7,10,20):\n",
    "        \n",
    "        q_percent = np.quantile(y_predicted, 1-sample_percent/100, axis=0)\n",
    "        y_true_p = np.where(y_predicted >= q_percent , y_true, 0)\n",
    "\n",
    "\n",
    "        n_bad_sample = y_true_p.sum()\n",
    "        p_bad_sample = n_bad_sample/n_bad\n",
    "        percent_bad_sample = round(p_bad_sample*100,2)\n",
    "        message += 'p_{}={},' . format(sample_percent, percent_bad_sample )   \n",
    "        \n",
    "        score += percent_bad_sample\n",
    "    \n",
    "    score = round(score, 2)     \n",
    "    message += ' total score={}'.format(score)   \n",
    "    \n",
    "    print(message)\n",
    "\n",
    "    if len(y_true) != n_test:\n",
    "        print('custom_scoring_p  return 0')\n",
    "        p_bad_sample = 0\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs_proba=True - label for geting probability\n",
    "score = make_scorer(custom_amount_score_p, greater_is_better=True, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, disable_default_eval_metric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'max_depth': range(2, 6), 'n_estimators': [500], 'learning_rate': np.arange(0.05, 0.5, 0.05), \n",
    "#          'subsample' : np.arange(0.5, 1, 0.05),  } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth': [2, 3, 5], 'learning_rate': [0.1, 0.2, 0.35], 'n_estimators': [60, 87, 110]}         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridsearch ps - for crossvalidation on train and test only without folds.\n",
    "n_train = train.shape[0]\n",
    "n_test = test.shape[0]\n",
    "test_fold = [1] *n_train + [0]* n_test\n",
    "ps = PredefinedSplit(test_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HalvingGridSearchCV - for default sampling dataset for threee folds: small, mean and big. It works with 'custom_score_p' methric.\n",
    "# clf = HalvingGridSearchCV(xgb_model, params, cv = ps, scoring=score)\n",
    "clf = GridSearchCV(xgb_model, params, cv = ps, scoring=score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params={ 'eval_metric': custom_amount_score_p, 'verbose': False, 'sample_weight': total_weight}\n",
    "# fit_params={\"early_stopping_rounds\":50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_1=2.7,p_2=9.8,p_3=11.28,p_4=12.83,p_5=14.56,p_6=16.69,p_7=18.5,p_10=25.04,p_20=44.18, total score=155.58\n",
      "custom_amount_score_p\n",
      "p_1=3.49,p_2=9.93,p_3=11.1,p_4=11.97,p_5=14.74,p_6=17.22,p_7=19.34,p_10=26.92,p_20=43.1, total score=157.81\n",
      "custom_amount_score_p\n",
      "p_1=5.18,p_2=9.6,p_3=11.02,p_4=12.69,p_5=14.7,p_6=18.72,p_7=18.96,p_10=25.94,p_20=43.85, total score=160.66\n",
      "custom_amount_score_p\n",
      "p_1=6.35,p_2=10.11,p_3=12.0,p_4=16.82,p_5=16.95,p_6=19.86,p_7=21.92,p_10=28.79,p_20=41.41, total score=174.21\n",
      "custom_amount_score_p\n",
      "p_1=6.88,p_2=12.31,p_3=12.53,p_4=15.32,p_5=18.06,p_6=21.15,p_7=26.03,p_10=30.57,p_20=42.18, total score=185.03\n",
      "custom_amount_score_p\n",
      "p_1=6.88,p_2=12.31,p_3=14.44,p_4=16.09,p_5=20.72,p_6=23.16,p_7=26.42,p_10=32.17,p_20=42.13, total score=194.32\n",
      "custom_amount_score_p\n",
      "p_1=5.71,p_2=13.45,p_3=14.93,p_4=15.94,p_5=18.31,p_6=20.57,p_7=21.56,p_10=31.35,p_20=46.9, total score=188.72\n",
      "custom_amount_score_p\n",
      "p_1=6.75,p_2=11.21,p_3=11.54,p_4=14.14,p_5=17.91,p_6=20.08,p_7=24.08,p_10=33.2,p_20=47.31, total score=186.22\n",
      "custom_amount_score_p\n",
      "p_1=6.74,p_2=10.24,p_3=11.11,p_4=14.97,p_5=18.04,p_6=21.38,p_7=24.65,p_10=33.79,p_20=47.42, total score=188.34\n",
      "custom_amount_score_p\n",
      "p_1=4.38,p_2=7.72,p_3=10.24,p_4=13.65,p_5=15.69,p_6=16.86,p_7=19.21,p_10=27.06,p_20=41.99, total score=156.8\n",
      "custom_amount_score_p\n",
      "p_1=4.23,p_2=7.35,p_3=11.96,p_4=14.52,p_5=17.4,p_6=21.5,p_7=23.55,p_10=27.46,p_20=43.19, total score=171.16\n",
      "custom_amount_score_p\n",
      "p_1=6.29,p_2=12.33,p_3=13.68,p_4=15.86,p_5=18.92,p_6=21.5,p_7=22.9,p_10=27.39,p_20=43.76, total score=182.63\n",
      "custom_amount_score_p\n",
      "p_1=8.74,p_2=11.52,p_3=16.16,p_4=16.66,p_5=20.56,p_6=22.6,p_7=26.59,p_10=31.16,p_20=42.24, total score=196.23\n",
      "custom_amount_score_p\n",
      "p_1=8.74,p_2=13.89,p_3=16.95,p_4=17.53,p_5=19.82,p_6=22.06,p_7=24.91,p_10=29.62,p_20=42.42, total score=195.94\n",
      "custom_amount_score_p\n",
      "p_1=8.99,p_2=13.56,p_3=17.42,p_4=18.59,p_5=19.75,p_6=22.03,p_7=25.58,p_10=28.2,p_20=42.09, total score=196.21\n",
      "custom_amount_score_p\n",
      "p_1=4.64,p_2=8.66,p_3=10.15,p_4=12.22,p_5=13.13,p_6=16.0,p_7=19.7,p_10=25.95,p_20=48.95, total score=159.4\n",
      "custom_amount_score_p\n",
      "p_1=4.67,p_2=8.53,p_3=10.65,p_4=11.88,p_5=12.9,p_6=13.25,p_7=17.9,p_10=23.8,p_20=41.79, total score=145.37\n",
      "custom_amount_score_p\n",
      "p_1=5.72,p_2=8.42,p_3=10.42,p_4=12.12,p_5=12.93,p_6=14.33,p_7=15.9,p_10=19.06,p_20=39.43, total score=138.33\n",
      "custom_amount_score_p\n",
      "p_1=7.68,p_2=12.62,p_3=13.88,p_4=14.89,p_5=17.91,p_6=22.93,p_7=25.08,p_10=29.02,p_20=42.23, total score=186.24\n",
      "custom_amount_score_p\n",
      "p_1=8.18,p_2=12.44,p_3=13.74,p_4=15.69,p_5=18.24,p_6=20.56,p_7=22.5,p_10=27.49,p_20=40.5, total score=179.34\n",
      "custom_amount_score_p\n",
      "p_1=7.74,p_2=11.93,p_3=13.51,p_4=16.88,p_5=18.47,p_6=18.82,p_7=24.57,p_10=28.61,p_20=40.16, total score=180.69\n",
      "custom_amount_score_p\n",
      "p_1=4.79,p_2=12.4,p_3=19.63,p_4=20.8,p_5=21.77,p_6=23.03,p_7=23.74,p_10=29.78,p_20=40.62, total score=196.56\n",
      "custom_amount_score_p\n",
      "p_1=8.6,p_2=15.67,p_3=20.0,p_4=21.84,p_5=22.96,p_6=23.87,p_7=26.31,p_10=30.77,p_20=42.11, total score=212.13\n",
      "custom_amount_score_p\n",
      "p_1=6.29,p_2=12.06,p_3=18.02,p_4=18.31,p_5=21.05,p_6=23.05,p_7=23.5,p_10=27.35,p_20=40.89, total score=190.52\n",
      "custom_amount_score_p\n",
      "p_1=4.56,p_2=8.81,p_3=12.19,p_4=13.1,p_5=13.9,p_6=16.49,p_7=19.03,p_10=23.64,p_20=39.29, total score=151.01\n",
      "custom_amount_score_p\n",
      "p_1=7.05,p_2=8.5,p_3=9.6,p_4=11.36,p_5=13.84,p_6=14.63,p_7=17.19,p_10=23.14,p_20=38.94, total score=144.25\n",
      "custom_amount_score_p\n",
      "p_1=6.3,p_2=8.23,p_3=10.21,p_4=12.46,p_5=15.97,p_6=16.65,p_7=18.44,p_10=23.59,p_20=33.96, total score=145.81\n",
      "custom_amount_score_p\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([1, 1, ..., 0, 0])),\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     disable_default_eval_metric=True,\n",
       "                                     gamma=None, gpu_id=None,\n",
       "                                     importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_...\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, use_label_encoder=False,\n",
       "                                     validate_parameters=None, verbosity=None),\n",
       "             param_grid={'learning_rate': [0.1, 0.2, 0.35],\n",
       "                         'max_depth': [2, 3, 5],\n",
       "                         'n_estimators': [60, 87, 110]},\n",
       "             scoring=make_scorer(custom_amount_score_p, needs_proba=True))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(total_train, total_label.astype(int),  **fit_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212.13\n",
      "{'learning_rate': 0.35, 'max_depth': 3, 'n_estimators': 87}\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_score_)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.values\n",
    "test = test.values\n",
    "label = db_teach.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_prediction =  AnalyzerPrediction(db_teach, db_test, white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_amount = None\n",
    "weight = analyzer_prediction.get_xgb_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_scores = ['count', 'amount']\n",
    "metric_score = metric_scores[1]\n",
    "\n",
    "config = clf.best_params_\n",
    "\n",
    "label = label.astype(int)\n",
    "\n",
    "model = xgb.XGBClassifier(**config,use_label_encoder=False, disable_default_eval_metric=True)\n",
    "\n",
    "model.fit(train, label, sample_weight=weight)\n",
    "\n",
    "test_pred = model.predict_proba(test)\n",
    "db_test[\"probability\"] = test_pred[:, 1]\n",
    "\n",
    "#config\n",
    "description = 'best_cv wiht wl-' + '-'.join([str(elem) for elem in (config['max_depth'], config['n_estimators'], config['learning_rate'], metric_score)])                \n",
    "result_df_amount = analyzer_prediction.get_table_prediction(description=description, result_df=result_df_amount, metric=metric_score)\n",
    "\n",
    "description = 'best_cv without wl-' + '-'.join([str(elem) for elem in (config['max_depth'], config['n_estimators'], config['learning_rate'], metric_score)])         \n",
    "analyzer_prediction.white_list = analyzer_prediction.get_empty_white_list()\n",
    "result_df_amount = analyzer_prediction.get_table_prediction(description=description, result_df=result_df_amount, metric=metric_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = result_df_amount.shape[0]\n",
    "sub_rows = list(range(n))[::2]\n",
    "stat_best = result_df_amount.copy().iloc[sub_rows,:]\n",
    "\n",
    "col_names = [col for col in stat_best.columns if col.startswith('p_') ] \n",
    "stat_best.loc[:, col_names] = stat_best.loc[:, col_names].astype(float)\n",
    "stat_best = stat_best.sort_values(by=\"rating\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>p_1</th>\n",
       "      <th>p_2</th>\n",
       "      <th>p_3</th>\n",
       "      <th>p_4</th>\n",
       "      <th>p_5</th>\n",
       "      <th>p_6</th>\n",
       "      <th>p_7</th>\n",
       "      <th>p_10</th>\n",
       "      <th>p_20</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best_cv wiht wl-3-87-0.35-amount</td>\n",
       "      <td>8.60</td>\n",
       "      <td>15.67</td>\n",
       "      <td>20.00</td>\n",
       "      <td>21.84</td>\n",
       "      <td>22.96</td>\n",
       "      <td>23.87</td>\n",
       "      <td>26.31</td>\n",
       "      <td>30.77</td>\n",
       "      <td>42.11</td>\n",
       "      <td>212.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>best_cv without wl-3-87-0.35-amount</td>\n",
       "      <td>6.35</td>\n",
       "      <td>12.99</td>\n",
       "      <td>17.42</td>\n",
       "      <td>20.09</td>\n",
       "      <td>21.66</td>\n",
       "      <td>23.53</td>\n",
       "      <td>25.35</td>\n",
       "      <td>28.57</td>\n",
       "      <td>40.50</td>\n",
       "      <td>196.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           description   p_1    p_2    p_3    p_4    p_5  \\\n",
       "0     best_cv wiht wl-3-87-0.35-amount  8.60  15.67  20.00  21.84  22.96   \n",
       "2  best_cv without wl-3-87-0.35-amount  6.35  12.99  17.42  20.09  21.66   \n",
       "\n",
       "     p_6    p_7   p_10   p_20  rating  \n",
       "0  23.87  26.31  30.77  42.11  212.13  \n",
       "2  23.53  25.35  28.57  40.50  196.46  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_best.iloc[:,:11] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This code dont working yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/cast42/xg-cv\n",
    "train_matrix = xgb.DMatrix(train, label=db_teach.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(missing=9999999999,\n",
    "                max_depth = 7,\n",
    "                n_estimators=700,\n",
    "                learning_rate=0.1, \n",
    "                nthread=4,\n",
    "                subsample=1.0,\n",
    "                colsample_bytree=0.5,\n",
    "                min_child_weight = 3,\n",
    "                seed=1301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param = clf.get_xgb_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvresult = xgb.cv(xgb_param, train_matrix, num_boost_round=1000, nfold=3, metrics=['logloss'], #feval= custom_metric_p5,\n",
    "     early_stopping_rounds=2, stratified=True, seed=1301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.603632</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.603381</td>\n",
       "      <td>0.000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.529977</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.530074</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.468865</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.468937</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.417367</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.417393</td>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.373389</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.373447</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.032552</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.048029</td>\n",
       "      <td>0.001153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.032549</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.048027</td>\n",
       "      <td>0.001154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.032535</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.048017</td>\n",
       "      <td>0.001153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.032521</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.048006</td>\n",
       "      <td>0.001160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.032491</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.001152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n",
       "0              0.603632           0.000030           0.603381   \n",
       "1              0.529977           0.000022           0.530074   \n",
       "2              0.468865           0.000081           0.468937   \n",
       "3              0.417367           0.000077           0.417393   \n",
       "4              0.373389           0.000042           0.373447   \n",
       "..                  ...                ...                ...   \n",
       "995            0.032552           0.000311           0.048029   \n",
       "996            0.032549           0.000311           0.048027   \n",
       "997            0.032535           0.000315           0.048017   \n",
       "998            0.032521           0.000310           0.048006   \n",
       "999            0.032491           0.000314           0.048000   \n",
       "\n",
       "     test-logloss-std  \n",
       "0            0.000038  \n",
       "1            0.000010  \n",
       "2            0.000024  \n",
       "3            0.000028  \n",
       "4            0.000062  \n",
       "..                ...  \n",
       "995          0.001153  \n",
       "996          0.001154  \n",
       "997          0.001153  \n",
       "998          0.001160  \n",
       "999          0.001152  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit on the trainingsdata\n",
      "[14:47:27] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
       "              min_child_weight=3, missing=9999999999, monotone_constraints='()',\n",
       "              n_estimators=1000, n_jobs=4, nthread=4, num_parallel_tree=1,\n",
       "              random_state=1301, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              seed=1301, subsample=1.0, tree_method='exact',\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.set_params(n_estimators=cvresult.shape[0])\n",
    "print('Fit on the trainingsdata')\n",
    "clf.fit(train, db_teach.status.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
