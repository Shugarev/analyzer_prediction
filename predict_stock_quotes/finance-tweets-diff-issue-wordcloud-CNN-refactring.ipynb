{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras in detais\n",
    "# http://www.100byte.ru/python/factors/factors.html\n",
    "\n",
    "# https://github.com/sismetanin/sentiment-analysis-of-tweets-in-russian/blob/master/Sentiment%20Analysis%20of%20Tweets%20in%20Russian%20using%20Convolutional%20Neural%20Networks.ipynb\n",
    "# https://habr.com/ru/company/mailru/blog/417767/ cnn example\n",
    "# https://realpython.com/sentiment-analysis-python/ for some information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/wordcards/stock-market-tweets-wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sergey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/sergey/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sergey/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# %load _header_import.py\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('df_tweets_wordcloud/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import collections\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter        \n",
    "\n",
    "# for autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21129, 8)\n",
      "(29006, 6)\n",
      "8078\n"
     ]
    }
   ],
   "source": [
    "# %load _load_target_data.py\n",
    "path_data = '/mnt/files/workdata/work/python-scripts/prediction_analyzer/predict_stock_quotes/data/'\n",
    "\n",
    "file_old = path_data + '21K-predict.csv'\n",
    "df_old = pd.read_csv(file_old,  dtype=str)\n",
    "\n",
    "print(df_old.shape)\n",
    "# (21129, 8)\n",
    "\n",
    "file_new = path_data + 'data-2021-06-10/trainingset _1_.xlsx'\n",
    "df_new = pd.read_excel(file_new, dtype=str)\n",
    "\n",
    "print(df_new.shape)\n",
    "# (29006, 6)\n",
    "\n",
    "df_old_sub = df_old[['title', 'Unnamed: 2']].copy()\n",
    "df_old_sub.columns = ['text', 'SENTIMENT']\n",
    "\n",
    "df_new_sub = df_new[['title', 'znak']].copy()\n",
    "df_new_sub.columns = ['text', 'SENTIMENT']\n",
    "\n",
    "mask = df_new_sub['text'].isin(df_old_sub['text'])\n",
    "df_unique =  df_new_sub[~ mask].copy()\n",
    "print(df_unique.shape[0])\n",
    "\n",
    "# df_new_sub - dataset c новыми данными\n",
    "\n",
    "# df_old_sub - исходный dataset\n",
    "# df_unique - dataset с отобранными новыми данными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_old_sub.shape=  21129\n",
      "df_old_sub.shape with correct sentiment=  20910\n",
      "df_new_sub.shape = 29006\n",
      "df_new_sub.shape = 28827\n",
      "df_unique.shape with correct sentiment=  7988\n",
      "df_old_sub.shape =  20910\n",
      "Counter({'positive': 9305, 'neutral': 8452, 'negative': 2937})\n",
      "df_old_sub.shape without duplicate =  20694\n",
      "df_unique.shape=  7988\n",
      "Counter({'neutral': 3545, 'positive': 3421, 'negative': 976})\n",
      "df_unique.shape without duplicate =  7942\n"
     ]
    }
   ],
   "source": [
    "# %load _prepare_sentiment_data.py\n",
    "_positive = 'positive'\n",
    "_negative = 'negative'\n",
    "_neutral = 'neutral'\n",
    "\n",
    "sentiment_list = [_neutral, _positive, _negative]\n",
    "replaced_dic = {'0': _neutral, '1': _positive, '2': _negative}\n",
    "\n",
    "#  ---df_old_sub\n",
    "df_old_sub['sentiment'] = df_old_sub['SENTIMENT'].replace(replaced_dic)\n",
    "print('df_old_sub.shape= ', df_old_sub.shape[0])\n",
    "\n",
    "mask = df_old_sub.sentiment.isin(sentiment_list)\n",
    "df_old_sub = df_old_sub[mask].copy()\n",
    "print('df_old_sub.shape with correct sentiment= ', df_old_sub.shape[0])\n",
    "\n",
    "#  ---df_new_sub\n",
    "df_new_sub['sentiment'] = df_new_sub['SENTIMENT'].replace(replaced_dic)\n",
    "print('df_new_sub.shape =', df_new_sub.shape[0])\n",
    "\n",
    "mask = df_new_sub.sentiment.isin(sentiment_list)\n",
    "df_new_sub = df_new_sub[mask].copy()\n",
    "print('df_new_sub.shape =', df_new_sub.shape[0])\n",
    "\n",
    "# ---create df_unique\n",
    "mask = df_new_sub['text'].isin(df_old_sub['text'])\n",
    "df_unique =  df_new_sub[~ mask].copy()\n",
    "\n",
    "print('df_unique.shape with correct sentiment= ', df_unique.shape[0])\n",
    "\n",
    "url_pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "def url(phrase):\n",
    "    return url_pattern.sub('', phrase)\n",
    "\n",
    "def prepare_data(dt: pd.DataFrame):\n",
    "    mask = dt.text.notnull()\n",
    "    dt = dt[mask].copy()\n",
    "\n",
    "    dt['text'] = dt['text'].apply(url)\n",
    "\n",
    "    dt.drop_duplicates(subset=['text'], keep='first', inplace=True)\n",
    "    print(Counter(dt['sentiment']))\n",
    "    \n",
    "    return dt\n",
    "\n",
    "# -- remove duplicate\n",
    "\n",
    "print('df_old_sub.shape = ', df_old_sub.shape[0])\n",
    "df_old_sub = prepare_data(df_old_sub)\n",
    "print('df_old_sub.shape without duplicate = ', df_old_sub.shape[0])\n",
    "\n",
    "print('df_unique.shape= ', df_unique.shape[0])\n",
    "df_unique = prepare_data(df_unique)\n",
    "print('df_unique.shape without duplicate = ', df_unique.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreProccess Text -------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sergey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/sergey/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sergey/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from _arrange import arrange_text\n",
    "# arrange create new clear column text2!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrange_text(df_old_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_old_sub - исходный dataset\n",
    "# df_unique - dataset с отобранными новыми данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old_sub.drop(columns='text', inplace=True)\n",
    "df_old_sub.rename(columns={'text2':'text'},inplace=True)\n",
    "df_old_sub.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrange_text(df_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique.drop(columns='text', inplace=True)\n",
    "df_unique.rename(columns={'text2':'text'},inplace=True)\n",
    "df_unique.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_old_sub "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick view of preprocessed tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load _word_cloud.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4f76a9dad686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dic =  collections.defaultdict(int)\n",
    "for text in df['text']:\n",
    "    for word in text.split():\n",
    "        word_dic[word] += 1\n",
    "\n",
    "word_df = pd.DataFrame.from_dict(word_dic, orient='index').rename(columns={0:'count'}).sort_values('count', ascending=False)  \n",
    "\n",
    "q = word_df['count'].quantile(0.75)\n",
    "mask = word_df['count'] >= q\n",
    "\n",
    "COUNT_WORDS_DEFAULT = word_df[mask].shape[0]\n",
    "COUNT_WORDS_DEFAULT = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Particularly noteworthy are the words in small fonts, such as \"volatility\", \"risk\", \"short interest\", \"covid\",..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------------------------ SPLIT TEACH TEST ------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_old_sub - исходный dataset\n",
    "# df_unique - dataset с отобранными новыми данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### --------------- constants for CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_dence -  3\n",
      "loss_func -  sparse_categorical_crossentropy\n",
      "last_activation -  softmax\n"
     ]
    }
   ],
   "source": [
    "research_sentiment = 'multiclass'\n",
    "\n",
    "last_dence = [1, 3][1]\n",
    "print('last_dence - ', last_dence)\n",
    "\n",
    "loss_func = ['binary_crossentropy', 'sparse_categorical_crossentropy', 'categorical_crossentropy'][1]\n",
    "print('loss_func - ', loss_func)\n",
    "\n",
    "last_activation = ['sigmoid', 'softmax'][1]\n",
    "print('last_activation - ', last_activation)\n",
    "\n",
    "epochs1 = 10\n",
    "epochs2 = 100\n",
    "\n",
    "split_only_df = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_finbert = {_positive:0, _negative:1,_neutral:2} \n",
    "\n",
    "df_old_sub['label'] = df_old_sub['sentiment'].replace(replaced_finbert)\n",
    "df_unique['label'] = df_unique['sentiment'].replace(replaced_finbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "dt_train, dt_test = train_test_split(df_old_sub, test_size=0.2, random_state=2)\n",
    "\n",
    "dt_train = pd.concat([dt_train, df_unique])#,df_new_body - experiment with body\n",
    "\n",
    "n_train = dt_train.shape[0]\n",
    "class_weight = { k:n_train/v for k,v in Counter(dt_train['label']).items() }\n",
    "\n",
    "x_train = dt_train['text'].values\n",
    "x_test = dt_test['text'].values\n",
    "\n",
    "y_train = dt_train['label'].values\n",
    "y_test = dt_test['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_weight -  {1: 7.4391132705739444, 0: 2.2563323201621075, 2: 2.3675461486421185}\n",
      "Counter({0: 10857, 2: 10347, 1: 3293})\n",
      "Counter({'positive': 10857, 'neutral': 10347, 'negative': 3293})\n",
      "Counter({0: 1869, 2: 1650, 1: 620})\n",
      "Counter({'positive': 1869, 'neutral': 1650, 'negative': 620})\n"
     ]
    }
   ],
   "source": [
    "print('class_weight - ', class_weight)\n",
    "print(Counter(y_train))\n",
    "print(Counter(dt_train['sentiment']))\n",
    "\n",
    "print(Counter(y_test))\n",
    "print(Counter(dt_test['sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SPLIT TEACH TEST by body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new_body = df_new[['body', 'znak']].copy()\n",
    "# print(df_new_body.shape[0])\n",
    "\n",
    "# df_new_body.columns = ['text', 'SENTIMENT']\n",
    "# mask = df_new_body.text.isnull()\n",
    "\n",
    "# df_new_body = df_new_body[~mask]\n",
    "# print(df_new_body.shape[0])\n",
    "\n",
    "# df_new_body['sentiment'] = df_new_body['SENTIMENT'].replace(replaced_dic)\n",
    "# mask = df_new_body.sentiment.isin(sentiment_list)\n",
    "\n",
    "# df_new_body = df_new_body[mask].copy()\n",
    "# print(df_new_body.shape[0])\n",
    "\n",
    "# # -- remove duplicate\n",
    "# df_new_body = prepare_data(df_new_body)\n",
    "# print(df_new_body.shape[0])\n",
    "\n",
    "# arrange_text(df_new_body)\n",
    "\n",
    "# df_new_body.drop(columns='text', inplace=True)\n",
    "# df_new_body.rename(columns={'text2':'text'},inplace=True)\n",
    "# df_new_body.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# replaced_finbert = {_positive:0, _negative:1,_neutral:2}\n",
    "# df_new_body['label'] = df_new_body['sentiment'].replace(replaced_finbert)\n",
    "\n",
    "# dt_train, dt_test = train_test_split(df_new_body, test_size=0.2, random_state=2)\n",
    "\n",
    "\n",
    "# n_train = dt_train.shape[0]\n",
    "# class_weight = { k:n_train/v for k,v in Counter(dt_train['label']).items() }\n",
    "\n",
    "# x_train = dt_train['text'].values\n",
    "# x_test = dt_test['text'].values\n",
    "\n",
    "# y_train = dt_train['label'].values\n",
    "# y_test = dt_test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Defining metrics¶\n",
    "#### Since Keras 2.0 metrics F-measure, precision, and recall have been removed, so the following code was found in the history of the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "\n",
    "from _keras_metrics import precision, recall, f1, matthews_correlation\n",
    "acc = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Preparing weights for the embedding layer\n",
    "#### I used Word2Vec [] embeddings, which were obtained at the previous step. It's a computationally efficient model for learning word embeddings developed\n",
    "#### y Google. The detailed guide of prepearing the embedding layer is availbale at https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sentence_len = max(df_old_sub['text'].apply(lambda x: len(x.split(' '))))\n",
    "max_sentence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# SENTENCE_LENGTH =  max_sentence_len\n",
    "SENTENCE_LENGTH = 43\n",
    "# NUM = 100000\n",
    "# NUM = COUNT_WORDS_DEFAULT\n",
    "NUM = 1500\n",
    "\n",
    "def get_sequences(tokenizer, x):\n",
    "    sequences = tokenizer.texts_to_sequences(x)\n",
    "    return pad_sequences(sequences, maxlen=SENTENCE_LENGTH)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train_seq = get_sequences(tokenizer, x_train)\n",
    "x_test_seq = get_sequences(tokenizer, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-06-15'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import date\n",
    "today = date.today()\n",
    "today = today.strftime(\"%Y-%m-%d\")\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/cnn_for_prod/tokenizer_model_2021-06-15\n"
     ]
    }
   ],
   "source": [
    "model_tokenizer_path = 'models/cnn_for_prod/tokenizer_model_{}'.format(today)\n",
    "print(model_tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   4,  26, 119],\n",
       "       [  0,   0,   0, ...,  14,  64,  35],\n",
       "       [  0,   0,   0, ...,  54, 446, 491],\n",
       "       ...,\n",
       "       [  0,   0,   0, ..., 582, 342, 449],\n",
       "       [  0,   0,   0, ...,   9,   9,  26],\n",
       "       [  0,   0,   0, ...,   9,   9,  26]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(tokenizer,model_tokenizer_path)\n",
    "\n",
    "tokenizer2 = joblib.load(model_tokenizer_path)\n",
    "\n",
    "x_test_seq2 = get_sequences(tokenizer2, x_test)\n",
    "x_test_seq2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model for Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/cnn_for_prod/word2vec_2021-06-15\n",
      "models/cnn_for_prod/tweets.txt\n"
     ]
    }
   ],
   "source": [
    "model_Word2Vec_path = 'models/cnn_for_prod/word2vec_{}'.format(today)\n",
    "print(model_Word2Vec_path)\n",
    "\n",
    "tweets_path = 'models/cnn_for_prod/tweets.txt'\n",
    "print(tweets_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tweets_path, 'w', encoding='utf-8') as f:\n",
    "    # Считываем тексты твитов \n",
    "    for row in dt_train['text'].astype('str').values:        \n",
    "        # Записываем предобработанные твиты в файл\n",
    "        print(row, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-15 11:43:25,158 : INFO : collecting all words and their counts\n",
      "2021-06-15 11:43:25,158 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-06-15 11:43:25,188 : INFO : PROGRESS: at sentence #10000, processed 97440 words, keeping 14237 word types\n",
      "2021-06-15 11:43:25,214 : INFO : PROGRESS: at sentence #20000, processed 197500 words, keeping 21044 word types\n",
      "2021-06-15 11:43:25,228 : INFO : collected 23715 word types from a corpus of 241862 raw words and 24497 sentences\n",
      "2021-06-15 11:43:25,228 : INFO : Loading a fresh vocabulary\n",
      "2021-06-15 11:43:25,239 : INFO : effective_min_count=3 retains 8833 unique words (37% of original 23715, drops 14882)\n",
      "2021-06-15 11:43:25,240 : INFO : effective_min_count=3 leaves 223236 word corpus (92% of original 241862, drops 18626)\n",
      "2021-06-15 11:43:25,254 : INFO : deleting the raw counts dictionary of 23715 items\n",
      "2021-06-15 11:43:25,254 : INFO : sample=0.001 downsamples 52 most-common words\n",
      "2021-06-15 11:43:25,255 : INFO : downsampling leaves estimated 185667 word corpus (83.2% of prior 223236)\n",
      "2021-06-15 11:43:25,265 : INFO : estimated required memory for 8833 words and 20 dimensions: 5829780 bytes\n",
      "2021-06-15 11:43:25,266 : INFO : resetting layer weights\n",
      "2021-06-15 11:43:26,400 : INFO : training model with 8 workers on 8833 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-06-15 11:43:26,518 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-06-15 11:43:26,520 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-06-15 11:43:26,522 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-06-15 11:43:26,523 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-06-15 11:43:26,526 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-06-15 11:43:26,527 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-06-15 11:43:26,530 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-06-15 11:43:26,531 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-06-15 11:43:26,531 : INFO : EPOCH - 1 : training on 241862 raw words (185398 effective words) took 0.1s, 1437558 effective words/s\n",
      "2021-06-15 11:43:26,638 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-06-15 11:43:26,642 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-06-15 11:43:26,642 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-06-15 11:43:26,645 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-06-15 11:43:26,646 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-06-15 11:43:26,649 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-06-15 11:43:26,650 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-06-15 11:43:26,652 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-06-15 11:43:26,653 : INFO : EPOCH - 2 : training on 241862 raw words (185773 effective words) took 0.1s, 1544867 effective words/s\n",
      "2021-06-15 11:43:26,763 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-06-15 11:43:26,764 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-06-15 11:43:26,769 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-06-15 11:43:26,773 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-06-15 11:43:26,773 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-06-15 11:43:26,775 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-06-15 11:43:26,777 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-06-15 11:43:26,779 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-06-15 11:43:26,780 : INFO : EPOCH - 3 : training on 241862 raw words (185741 effective words) took 0.1s, 1473862 effective words/s\n",
      "2021-06-15 11:43:26,894 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-06-15 11:43:26,896 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-06-15 11:43:26,897 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-06-15 11:43:26,898 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-06-15 11:43:26,902 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-06-15 11:43:26,903 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-06-15 11:43:26,904 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-06-15 11:43:26,905 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-06-15 11:43:26,905 : INFO : EPOCH - 4 : training on 241862 raw words (185562 effective words) took 0.1s, 1493789 effective words/s\n",
      "2021-06-15 11:43:27,012 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-06-15 11:43:27,014 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-06-15 11:43:27,018 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-06-15 11:43:27,019 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-06-15 11:43:27,025 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-06-15 11:43:27,026 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-06-15 11:43:27,027 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-06-15 11:43:27,028 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-06-15 11:43:27,028 : INFO : EPOCH - 5 : training on 241862 raw words (185904 effective words) took 0.1s, 1530581 effective words/s\n",
      "2021-06-15 11:43:27,028 : INFO : training on a 1209310 raw words (928378 effective words) took 0.6s, 1479704 effective words/s\n",
      "2021-06-15 11:43:27,029 : INFO : saving Word2Vec object under models/cnn_for_prod/word2vec_2021-06-15, separately None\n",
      "2021-06-15 11:43:27,029 : INFO : not storing attribute vectors_norm\n",
      "2021-06-15 11:43:27,030 : INFO : not storing attribute cum_table\n",
      "2021-06-15 11:43:27,047 : INFO : saved models/cnn_for_prod/word2vec_2021-06-15\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import multiprocessing\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Считываем файл с предобработанными твитами\n",
    "data = gensim.models.word2vec.LineSentence(tweets_path)\n",
    "\n",
    "# Обучаем модель \n",
    "size = 20#200\n",
    "model = Word2Vec(data, size=size, window=5, min_count=3, workers=multiprocessing.cpu_count(), seed=123)\n",
    "\n",
    "model.save(model_Word2Vec_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-15 11:43:33,604 : INFO : loading Word2Vec object from models/cnn_for_prod/word2vec_2021-06-15\n",
      "2021-06-15 11:43:33,697 : INFO : loading wv recursively from models/cnn_for_prod/word2vec_2021-06-15.wv.* with mmap=None\n",
      "2021-06-15 11:43:33,698 : INFO : setting ignored attribute vectors_norm to None\n",
      "2021-06-15 11:43:33,699 : INFO : loading vocabulary recursively from models/cnn_for_prod/word2vec_2021-06-15.vocabulary.* with mmap=None\n",
      "2021-06-15 11:43:33,699 : INFO : loading trainables recursively from models/cnn_for_prod/word2vec_2021-06-15.trainables.* with mmap=None\n",
      "2021-06-15 11:43:33,699 : INFO : setting ignored attribute cum_table to None\n",
      "2021-06-15 11:43:33,700 : INFO : loaded models/cnn_for_prod/word2vec_2021-06-15\n"
     ]
    }
   ],
   "source": [
    "# Загружаем обученную модель\n",
    "w2v_model = Word2Vec.load(model_Word2Vec_path)\n",
    "DIM = w2v_model.vector_size \n",
    "# Инициализируем матрицу embedding слоя нулями\n",
    "embedding_matrix = np.zeros((NUM, DIM))\n",
    "# Добавляем NUM=100000 наиболее часто встречающихся слов из обучающей выборки в embedding слой\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= NUM:\n",
    "        break\n",
    "    if word in w2v_model.wv.vocab.keys():\n",
    "        embedding_matrix[i] = w2v_model.wv[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Building the CNN¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import concatenate\n",
    "\n",
    "tweet_input = Input(shape=(SENTENCE_LENGTH,), dtype='int32')\n",
    "tweet_encoder = Embedding(NUM, DIM, input_length=SENTENCE_LENGTH, weights=[embedding_matrix], trainable=False)(tweet_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В разработанной архитектуре использованы фильтры с высотой h=(2, 3, 4, 5), которые предназначены для параллельной обработки биграмм, триграмм, 4-грамм и 5-грамм соответственно. \n",
    "Добавил в нейронную сеть по 10 свёрточных слоев для каждой высоты фильтра, функция активации — ReLU. С рекомендациями по поиску оптимальной высоты и количества фильтров можно ознакомиться в работе [2].\n",
    "\n",
    "После обработки слоями свертки, карты признаков поступали на слои субдискретизации, где к ним применялась операция 1-max-pooling, тем самым извлекая наиболее значимые n-граммы из текста. \n",
    "На следующем этапе происходило объединение в общий вектор признаков (слой объединения), который подавался в скрытый полносвязный слой с 30 нейронами.\n",
    "На последнем этапе итоговая карта признаков подавалась на выходной слой нейронной сети с сигмоидальной функцией активации.\n",
    "\n",
    "Поскольку нейронные сети склонны к переобучению, после embedding-слоя и перед скрытым полносвязным слоем я добавил dropout-регуляризацию c вероятностью выброса вершины p=0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, concatenate, Activation, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "\n",
    "branches = []\n",
    "x = Dropout(0.2)(tweet_encoder)\n",
    "\n",
    "for size, filters_count in [(2, 10), (3, 10), (4, 10), (5, 10), (6,10), (7,10)]: \n",
    "    for i in range(filters_count):\n",
    "        # Добавляем слой свертки\n",
    "        branch = Conv1D(filters=1, kernel_size=size, padding='valid', activation='relu')(x)\n",
    "        branch = GlobalMaxPooling1D()(branch)\n",
    "        branches.append(branch)\n",
    "\n",
    "x = concatenate(branches, axis=1)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(30, activation='relu')(x)\n",
    "x = Dense(last_dence)(x)\n",
    "output = Activation(last_activation)(x) \n",
    "\n",
    "\n",
    "model = Model(inputs=[tweet_input], outputs=[output])\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[precision, recall, f1, matthews_correlation])\n",
    "model.compile(loss=loss_func, optimizer='adam', metrics=['accuracy']) # matthews_correlation adam\n",
    "model._name = research_sentiment\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training and evaluating the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The dataset was divided into three parts: train dataset (60% of the entire dataset), validation dataset (20% of the entire dataset), and test dataset\n",
    "##### (20% of the entire dataset). The loss function was minimized using the Adam optimizer with a learning rate of 0.001. The embedding layer, which was \n",
    "##### initialized with Word2Vec word embeddings, was frozen for the first 10 epochs. Then we train model from the previous step with best validation scores \n",
    "##### for additional 5 epochs with unfrozen embeddings and a learning rate of 0.0001. The best results in terms of F-measure was 77.67%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На первом этапе обучения заморозил embedding-слой, все остальные слои обучались в течение 10 эпох:\n",
    "\n",
    "Размер группы примеров, используемых для обучения: 32.\n",
    "Размер валидационной выборки: 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = K.constant(y_train)\n",
    "y_test = K.constant(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 7.4391132705739444, 0: 2.2563323201621075, 2: 2.3675461486421185}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weight\n",
    "# {1: 7.145015105740181, 0: 2.2263313609467454, 2: 2.43384298735666}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_weight = np.ones(shape=(len(y_train),))\n",
    "# sample_weight[y_train == 1] = len(y_train[y_train == 0]) / len(y_train[y_train == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-15 11:44:48,930 : WARNING : `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "575/575 [==============================] - 7s 7ms/step - loss: 3.4078 - accuracy: 0.4490 - val_loss: 0.9393 - val_accuracy: 0.5455\n",
      "Epoch 2/10\n",
      "575/575 [==============================] - 3s 6ms/step - loss: 2.8200 - accuracy: 0.5432 - val_loss: 0.9161 - val_accuracy: 0.5731\n",
      "Epoch 3/10\n",
      "575/575 [==============================] - 3s 6ms/step - loss: 2.7501 - accuracy: 0.5620 - val_loss: 0.9054 - val_accuracy: 0.5843\n",
      "Epoch 4/10\n",
      "575/575 [==============================] - 3s 6ms/step - loss: 2.6988 - accuracy: 0.5670 - val_loss: 0.8846 - val_accuracy: 0.6171\n",
      "Epoch 5/10\n",
      "575/575 [==============================] - 3s 6ms/step - loss: 2.6740 - accuracy: 0.5748 - val_loss: 0.9036 - val_accuracy: 0.5791\n",
      "Epoch 6/10\n",
      "575/575 [==============================] - 3s 6ms/step - loss: 2.6680 - accuracy: 0.5780 - val_loss: 0.8874 - val_accuracy: 0.5736\n",
      "Epoch 7/10\n",
      "575/575 [==============================] - 3s 6ms/step - loss: 2.6119 - accuracy: 0.5862 - val_loss: 0.8762 - val_accuracy: 0.5948\n",
      "Epoch 8/10\n",
      "575/575 [==============================] - 3s 6ms/step - loss: 2.5895 - accuracy: 0.5830 - val_loss: 0.8734 - val_accuracy: 0.5882\n",
      "Epoch 9/10\n",
      "575/575 [==============================] - 3s 6ms/step - loss: 2.5774 - accuracy: 0.5775 - val_loss: 0.8817 - val_accuracy: 0.5768\n",
      "Epoch 10/10\n",
      "575/575 [==============================] - 3s 6ms/step - loss: 2.5460 - accuracy: 0.5829 - val_loss: 0.8339 - val_accuracy: 0.6196\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"models/cnn/cnn-frozen-embed-{}\".format(model.name) + \"-{epoch:02d}-{val_accuracy:.4f}.hdf5\",\n",
    "                             monitor='val_accuracy', save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "# history = model.fit(x_train_seq, y_train, batch_size=32, epochs=epochs1, validation_split=0.25, sample_weight=sample_weight, callbacks = [checkpoint])\n",
    "\n",
    "history = model.fit(x_train_seq, y_train, batch_size=32, epochs=epochs1, validation_split=0.25, class_weight=class_weight, callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем выбрал модель с наивысшими показателями F-меры на валидационном наборе данных, т.е. модель, полученную на восьмой эпохе обучения (F1=0.7791). \n",
    "\n",
    "У модели разморозил embedding-слой, после чего запустил еще пять эпох обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[1].trainable = True\n",
    "\n",
    "adam = optimizers.Adam(lr=0.0001)\n",
    "# rmsprop = optimizers.RMSprop(lr=0.001)\n",
    "\n",
    "\n",
    "model.compile(loss=loss_func, optimizer=adam, metrics=['accuracy']) # matthews_correlation  precision, recall, f1, acc\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 7.4391132705739444, 0: 2.2563323201621075, 2: 2.3675461486421185}\n",
      "epoche2 100\n"
     ]
    }
   ],
   "source": [
    "print(class_weight)\n",
    "print('epoche2', epochs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-15 11:46:28,665 : WARNING : `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "575/575 [==============================] - 8s 9ms/step - loss: 2.5443 - accuracy: 0.5981 - val_loss: 0.8441 - val_accuracy: 0.6020\n",
      "Epoch 2/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 2.4961 - accuracy: 0.5968 - val_loss: 0.8396 - val_accuracy: 0.6023\n",
      "Epoch 3/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 2.4540 - accuracy: 0.6039 - val_loss: 0.8344 - val_accuracy: 0.6029\n",
      "Epoch 4/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 2.3910 - accuracy: 0.6162 - val_loss: 0.8195 - val_accuracy: 0.6168\n",
      "Epoch 5/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 2.3158 - accuracy: 0.6403 - val_loss: 0.8081 - val_accuracy: 0.6431\n",
      "Epoch 6/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 2.2770 - accuracy: 0.6663 - val_loss: 0.7923 - val_accuracy: 0.6518\n",
      "Epoch 7/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 2.2217 - accuracy: 0.6714 - val_loss: 0.7793 - val_accuracy: 0.6511\n",
      "Epoch 8/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 2.1786 - accuracy: 0.6726 - val_loss: 0.7666 - val_accuracy: 0.6522\n",
      "Epoch 9/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 2.1128 - accuracy: 0.6896 - val_loss: 0.7589 - val_accuracy: 0.6555\n",
      "Epoch 10/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 2.0822 - accuracy: 0.6923 - val_loss: 0.7457 - val_accuracy: 0.6637\n",
      "Epoch 11/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 2.0244 - accuracy: 0.7056 - val_loss: 0.7350 - val_accuracy: 0.6676\n",
      "Epoch 12/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 2.0292 - accuracy: 0.6968 - val_loss: 0.7281 - val_accuracy: 0.6738\n",
      "Epoch 13/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.9691 - accuracy: 0.7067 - val_loss: 0.7268 - val_accuracy: 0.6766\n",
      "Epoch 14/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 2.0004 - accuracy: 0.7054 - val_loss: 0.7155 - val_accuracy: 0.6864\n",
      "Epoch 15/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.9159 - accuracy: 0.7187 - val_loss: 0.7087 - val_accuracy: 0.6942\n",
      "Epoch 16/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.8704 - accuracy: 0.7328 - val_loss: 0.7016 - val_accuracy: 0.6970\n",
      "Epoch 17/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.9012 - accuracy: 0.7268 - val_loss: 0.6967 - val_accuracy: 0.6993\n",
      "Epoch 18/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.8589 - accuracy: 0.7387 - val_loss: 0.6917 - val_accuracy: 0.7011\n",
      "Epoch 19/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.7992 - accuracy: 0.7423 - val_loss: 0.6869 - val_accuracy: 0.7071\n",
      "Epoch 20/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.8181 - accuracy: 0.7390 - val_loss: 0.6772 - val_accuracy: 0.7127\n",
      "Epoch 21/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.7898 - accuracy: 0.7522 - val_loss: 0.6800 - val_accuracy: 0.7096\n",
      "Epoch 22/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.7519 - accuracy: 0.7594 - val_loss: 0.6786 - val_accuracy: 0.7092\n",
      "Epoch 23/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.6987 - accuracy: 0.7627 - val_loss: 0.6755 - val_accuracy: 0.7148\n",
      "Epoch 24/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.7226 - accuracy: 0.7656 - val_loss: 0.6687 - val_accuracy: 0.7156\n",
      "Epoch 25/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.7048 - accuracy: 0.7684 - val_loss: 0.6709 - val_accuracy: 0.7143\n",
      "Epoch 26/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.6826 - accuracy: 0.7698 - val_loss: 0.6652 - val_accuracy: 0.7207\n",
      "Epoch 27/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.6750 - accuracy: 0.7698 - val_loss: 0.6609 - val_accuracy: 0.7234\n",
      "Epoch 28/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.6882 - accuracy: 0.7707 - val_loss: 0.6650 - val_accuracy: 0.7198\n",
      "Epoch 29/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.6498 - accuracy: 0.7757 - val_loss: 0.6577 - val_accuracy: 0.7246\n",
      "Epoch 30/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.6214 - accuracy: 0.7750 - val_loss: 0.6581 - val_accuracy: 0.7246\n",
      "Epoch 31/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.6267 - accuracy: 0.7803 - val_loss: 0.6528 - val_accuracy: 0.7285\n",
      "Epoch 32/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.5898 - accuracy: 0.7858 - val_loss: 0.6548 - val_accuracy: 0.7283\n",
      "Epoch 33/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.5452 - accuracy: 0.7901 - val_loss: 0.6586 - val_accuracy: 0.7260\n",
      "Epoch 34/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.5451 - accuracy: 0.7874 - val_loss: 0.6536 - val_accuracy: 0.7290\n",
      "Epoch 35/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.5410 - accuracy: 0.7892 - val_loss: 0.6505 - val_accuracy: 0.7303\n",
      "Epoch 36/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.5478 - accuracy: 0.7843 - val_loss: 0.6482 - val_accuracy: 0.7334\n",
      "Epoch 37/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.5213 - accuracy: 0.7978 - val_loss: 0.6500 - val_accuracy: 0.7321\n",
      "Epoch 38/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.5298 - accuracy: 0.7914 - val_loss: 0.6465 - val_accuracy: 0.7365\n",
      "Epoch 39/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.4874 - accuracy: 0.7977 - val_loss: 0.6484 - val_accuracy: 0.7357\n",
      "Epoch 40/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.4958 - accuracy: 0.7925 - val_loss: 0.6482 - val_accuracy: 0.7370\n",
      "Epoch 41/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.4695 - accuracy: 0.7948 - val_loss: 0.6487 - val_accuracy: 0.7360\n",
      "Epoch 42/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.4674 - accuracy: 0.7951 - val_loss: 0.6403 - val_accuracy: 0.7419\n",
      "Epoch 43/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.4926 - accuracy: 0.7966 - val_loss: 0.6453 - val_accuracy: 0.7350\n",
      "Epoch 44/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.4794 - accuracy: 0.8000 - val_loss: 0.6380 - val_accuracy: 0.7409\n",
      "Epoch 45/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.4396 - accuracy: 0.8064 - val_loss: 0.6448 - val_accuracy: 0.7398\n",
      "Epoch 46/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.4343 - accuracy: 0.8029 - val_loss: 0.6379 - val_accuracy: 0.7438\n",
      "Epoch 47/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.4165 - accuracy: 0.8079 - val_loss: 0.6365 - val_accuracy: 0.7442\n",
      "Epoch 48/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.4407 - accuracy: 0.8048 - val_loss: 0.6354 - val_accuracy: 0.7448\n",
      "Epoch 49/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.4461 - accuracy: 0.8049 - val_loss: 0.6369 - val_accuracy: 0.7464\n",
      "Epoch 50/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.3719 - accuracy: 0.8136 - val_loss: 0.6375 - val_accuracy: 0.7456\n",
      "Epoch 51/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.4089 - accuracy: 0.8048 - val_loss: 0.6335 - val_accuracy: 0.7499\n",
      "Epoch 52/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.3939 - accuracy: 0.8140 - val_loss: 0.6381 - val_accuracy: 0.7489\n",
      "Epoch 53/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.3762 - accuracy: 0.8130 - val_loss: 0.6326 - val_accuracy: 0.7510\n",
      "Epoch 54/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.3800 - accuracy: 0.8187 - val_loss: 0.6372 - val_accuracy: 0.7466\n",
      "Epoch 55/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.3854 - accuracy: 0.8070 - val_loss: 0.6292 - val_accuracy: 0.7517\n",
      "Epoch 56/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.3468 - accuracy: 0.8202 - val_loss: 0.6304 - val_accuracy: 0.7525\n",
      "Epoch 57/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.3308 - accuracy: 0.8209 - val_loss: 0.6357 - val_accuracy: 0.7520\n",
      "Epoch 58/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.3151 - accuracy: 0.8197 - val_loss: 0.6299 - val_accuracy: 0.7528\n",
      "Epoch 59/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.3484 - accuracy: 0.8133 - val_loss: 0.6315 - val_accuracy: 0.7531\n",
      "Epoch 60/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.3073 - accuracy: 0.8263 - val_loss: 0.6302 - val_accuracy: 0.7544\n",
      "Epoch 61/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.3165 - accuracy: 0.8249 - val_loss: 0.6246 - val_accuracy: 0.7554\n",
      "Epoch 62/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.2910 - accuracy: 0.8256 - val_loss: 0.6296 - val_accuracy: 0.7528\n",
      "Epoch 63/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.3036 - accuracy: 0.8189 - val_loss: 0.6252 - val_accuracy: 0.7549\n",
      "Epoch 64/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.2628 - accuracy: 0.8237 - val_loss: 0.6294 - val_accuracy: 0.7576\n",
      "Epoch 65/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.3148 - accuracy: 0.8170 - val_loss: 0.6338 - val_accuracy: 0.7538\n",
      "Epoch 66/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.2708 - accuracy: 0.8276 - val_loss: 0.6294 - val_accuracy: 0.7551\n",
      "Epoch 67/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.2801 - accuracy: 0.8292 - val_loss: 0.6339 - val_accuracy: 0.7527\n",
      "Epoch 68/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.2792 - accuracy: 0.8228 - val_loss: 0.6250 - val_accuracy: 0.7543\n",
      "Epoch 69/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.1997 - accuracy: 0.8389 - val_loss: 0.6264 - val_accuracy: 0.7548\n",
      "Epoch 70/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.2617 - accuracy: 0.8310 - val_loss: 0.6292 - val_accuracy: 0.7538\n",
      "Epoch 71/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.2223 - accuracy: 0.8315 - val_loss: 0.6349 - val_accuracy: 0.7530\n",
      "Epoch 72/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.2560 - accuracy: 0.8315 - val_loss: 0.6331 - val_accuracy: 0.7544\n",
      "Epoch 73/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.2319 - accuracy: 0.8309 - val_loss: 0.6302 - val_accuracy: 0.7556\n",
      "Epoch 74/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.2300 - accuracy: 0.8316 - val_loss: 0.6326 - val_accuracy: 0.7544\n",
      "Epoch 75/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.2290 - accuracy: 0.8324 - val_loss: 0.6312 - val_accuracy: 0.7548\n",
      "Epoch 76/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.2612 - accuracy: 0.8295 - val_loss: 0.6340 - val_accuracy: 0.7553\n",
      "Epoch 77/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.2045 - accuracy: 0.8337 - val_loss: 0.6376 - val_accuracy: 0.7525\n",
      "Epoch 78/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.2079 - accuracy: 0.8314 - val_loss: 0.6401 - val_accuracy: 0.7525\n",
      "Epoch 79/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.2328 - accuracy: 0.8317 - val_loss: 0.6398 - val_accuracy: 0.7546\n",
      "Epoch 80/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.1881 - accuracy: 0.8407 - val_loss: 0.6348 - val_accuracy: 0.7558\n",
      "Epoch 81/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.1732 - accuracy: 0.8444 - val_loss: 0.6407 - val_accuracy: 0.7540\n",
      "Epoch 82/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.1987 - accuracy: 0.8352 - val_loss: 0.6403 - val_accuracy: 0.7559\n",
      "Epoch 83/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.1893 - accuracy: 0.8427 - val_loss: 0.6379 - val_accuracy: 0.7574\n",
      "Epoch 84/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.1822 - accuracy: 0.8369 - val_loss: 0.6407 - val_accuracy: 0.7535\n",
      "Epoch 85/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.1707 - accuracy: 0.8458 - val_loss: 0.6438 - val_accuracy: 0.7540\n",
      "Epoch 86/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.1940 - accuracy: 0.8415 - val_loss: 0.6386 - val_accuracy: 0.7562\n",
      "Epoch 87/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.1910 - accuracy: 0.8386 - val_loss: 0.6382 - val_accuracy: 0.7574\n",
      "Epoch 88/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.1602 - accuracy: 0.8446 - val_loss: 0.6430 - val_accuracy: 0.7561\n",
      "Epoch 89/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.1974 - accuracy: 0.8413 - val_loss: 0.6412 - val_accuracy: 0.7569\n",
      "Epoch 90/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.1587 - accuracy: 0.8429 - val_loss: 0.6454 - val_accuracy: 0.7561\n",
      "Epoch 91/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.1365 - accuracy: 0.8436 - val_loss: 0.6480 - val_accuracy: 0.7554\n",
      "Epoch 92/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.1148 - accuracy: 0.8462 - val_loss: 0.6467 - val_accuracy: 0.7559\n",
      "Epoch 93/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.1545 - accuracy: 0.8424 - val_loss: 0.6434 - val_accuracy: 0.7571\n",
      "Epoch 94/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.1419 - accuracy: 0.8429 - val_loss: 0.6458 - val_accuracy: 0.7585\n",
      "Epoch 95/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.1212 - accuracy: 0.8468 - val_loss: 0.6462 - val_accuracy: 0.7558\n",
      "Epoch 96/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.1391 - accuracy: 0.8412 - val_loss: 0.6496 - val_accuracy: 0.7561\n",
      "Epoch 97/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.1203 - accuracy: 0.8480 - val_loss: 0.6529 - val_accuracy: 0.7541\n",
      "Epoch 98/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.0940 - accuracy: 0.8510 - val_loss: 0.6482 - val_accuracy: 0.7569\n",
      "Epoch 99/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.1483 - accuracy: 0.8426 - val_loss: 0.6479 - val_accuracy: 0.7564\n",
      "Epoch 100/100\n",
      "575/575 [==============================] - 5s 8ms/step - loss: 1.1550 - accuracy: 0.8401 - val_loss: 0.6517 - val_accuracy: 0.7528\n"
     ]
    }
   ],
   "source": [
    "# for i in range(5):\n",
    "checkpoint = ModelCheckpoint(\"models/cnn/loop-{}-cnn-frozen-embed-{}\".format(i, model.name) + \"-{epoch:02d}-{val_accuracy:.4f}.hdf5\", \n",
    "                             monitor='val_accuracy', save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "# history = model.fit(x_train_seq, y_train, batch_size=32, epochs=epochs2, validation_split=0.25, shuffle=True, \n",
    "#                     sample_weight=sample_weight, callbacks = [checkpoint])  \n",
    "history = model.fit(x_train_seq, y_train, batch_size=32, epochs=epochs2, validation_split=0.25, class_weight=class_weight, \n",
    "                    shuffle=True, callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict_cnn(x):\n",
    "    if x[0] - x[1] > x[2]:\n",
    "        return 0\n",
    "    elif x[1] - x[0] > x[2]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction like finbert\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    0.87418   0.78812   0.82893      1869\n",
      "         1.0    0.71664   0.77097   0.74281       620\n",
      "         2.0    0.77616   0.84061   0.80710      1650\n",
      "\n",
      "    accuracy                        0.80647      4139\n",
      "   macro avg    0.78900   0.79990   0.79295      4139\n",
      "weighted avg    0.81151   0.80647   0.80733      4139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_proba = model.predict(x_test_seq)\n",
    "dt_test['predict_proba'] = predict_proba.tolist()\n",
    "dt_test['predict_label'] = dt_test.predict_proba.apply(lambda x: get_predict_cnn(x))\n",
    "print('prediction like finbert')\n",
    "print(classification_report(y_test, dt_test['predict_label'] , digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction like finbert\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#          0.0    0.86520   0.77956   0.82015      1869\n",
    "#          1.0    0.72457   0.74677   0.73550       620\n",
    "#          2.0    0.76432   0.84121   0.80092      1650\n",
    "\n",
    "#     accuracy                        0.79923      4139\n",
    "#    macro avg    0.78470   0.78918   0.78553      4139\n",
    "# weighted avg    0.80392   0.79923   0.79981      4139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    0.85706   0.81808   0.83712      1869\n",
      "         1.0    0.67175   0.78226   0.72280       620\n",
      "         2.0    0.81629   0.80788   0.81206      1650\n",
      "\n",
      "    accuracy                        0.80865      4139\n",
      "   macro avg    0.78170   0.80274   0.79066      4139\n",
      "weighted avg    0.81305   0.80865   0.81001      4139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_proba = model.predict(x_test_seq)\n",
    "dt_test['predict_proba'] = predict_proba.tolist()\n",
    "dt_test['predict_label'] = dt_test.predict_proba.apply(lambda x: np.argmax(x,axis=0))\n",
    "print(classification_report(y_test, dt_test['predict_label'] , digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#            precision    recall  f1-score   support\n",
    "\n",
    "#          0.0    0.85166   0.80792   0.82921      1869\n",
    "#          1.0    0.66855   0.76452   0.71332       620\n",
    "#          2.0    0.80688   0.81030   0.80859      1650\n",
    "\n",
    "#     accuracy                        0.80237      4139\n",
    "#    macro avg    0.77570   0.79425   0.78371      4139\n",
    "# weighted avg    0.80638   0.80237   0.80363      4139"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_path = 'models/cnn_for_prod/keras_cnn_{}_model_{}.hdf5'.format(research_sentiment,today)\n",
    "print(cnn_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(cnn_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model2 = load_model(cnn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba = model2.predict(x_test_seq)\n",
    "dt_test['predict_proba'] = predict_proba.tolist()\n",
    "dt_test['predict_label'] = dt_test.predict_proba.apply(lambda x: np.argmax(x,axis=0))\n",
    "print(classification_report(y_test, dt_test['predict_label'] , digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrain -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid = df_unique['text'].values\n",
    "y_valid = df_unique['label'].values\n",
    "\n",
    "x_valid_seq = get_sequences(tokenizer2, x_valid)\n",
    "\n",
    "y_valid = K.constant(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_unique = df_unique.shape[0]\n",
    "class_weight_valid = { k:n_train/v for k,v in Counter(df_unique['label']).items() }\n",
    "class_weight_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_name = 'multiclass_v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Заморозить первый слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.layers[1].trainable = False\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"models/cnn/cnn-frozen-embed-{}\".format(model2_name) + \"-{epoch:02d}-{val_accuracy:.4f}.hdf5\",\n",
    "                             monitor='val_accuracy', save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "history = model2.fit(x_valid_seq, y_valid, batch_size=32, epochs=epochs1, validation_split=0.25, class_weight=class_weight_valid, callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Разморозить первый слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.layers[1].trainable = True\n",
    "\n",
    "adam = optimizers.Adam(lr=0.0001)\n",
    "\n",
    "model2.compile(loss=loss_func, optimizer=adam, metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "epochs3 = 25\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"models/cnn/loop-{}-cnn-frozen-embed-{}\".format(i, model2_name) + \"-{epoch:02d}-{val_accuracy:.4f}.hdf5\", \n",
    "                             monitor='val_accuracy', save_best_only=True, mode='auto', period=1)\n",
    " \n",
    "history = model2.fit(x_train_seq, y_train, batch_size=32, epochs=epochs3, validation_split=0.25, class_weight=class_weight_valid, \n",
    "                    shuffle=True, callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#          precision    recall  f1-score   support\n",
    "\n",
    "#          0.0    0.85166   0.80792   0.82921      1869\n",
    "#          1.0    0.66855   0.76452   0.71332       620\n",
    "#          2.0    0.80688   0.81030   0.80859      1650\n",
    "\n",
    "#     accuracy                        0.80237      4139\n",
    "#    macro avg    0.77570   0.79425   0.78371      4139\n",
    "# weighted avg    0.80638   0.80237   0.80363      4139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba = model2.predict(x_test_seq)\n",
    "dt_test['predict_proba'] = predict_proba.tolist()\n",
    "dt_test['predict_label'] = dt_test.predict_proba.apply(lambda x: np.argmax(x,axis=0))\n",
    "print(classification_report(y_test, dt_test['predict_label'] , digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba = model.predict(x_test_seq)\n",
    "dt_test['predict_proba'] = predict_proba.tolist()\n",
    "dt_test['predict_label'] = dt_test.predict_proba.apply(lambda x: get_predict_cnn(x))\n",
    "print('prediction like finbert')\n",
    "print(classification_report(y_test, dt_test['predict_label'] , digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Remove code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba = model(x_test_seq).numpy()\n",
    "test['predict_proba'] = predict_proba.tolist()\n",
    "test['predict_label'] = test.predict_proba.apply(lambda x: np.argmax(x,axis=0))\n",
    "print(classification_report(y_test, test['predict_label'] , digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '/mnt/files/workdata/work/python-scripts/prediction_analyzer/predict-stock-quotes/data/' \n",
    "file_path = path_data + 'news_dump_predicted.csv'\n",
    "news_dump_predicted = pd.read_csv(file_path,  dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_dump_predicted.shape[0])\n",
    "news_dump_predicted.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dump_predicted['text2'] = news_dump_predicted['text2'].astype('str')\n",
    "x_test2 = news_dump_predicted['text2'].values\n",
    "x_test_seq2 = get_sequences(tokenizer, x_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba_tensor = model(x_test_seq2)\n",
    "predict_proba = predict_proba_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dump_predicted['proba_cnn'] = predict_proba.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dump_predicted['label_cnn'] = news_dump_predicted['proba_cnn'].apply(lambda x: np.argmax(x,axis=0))\n",
    "news_dump_predicted['sentiment_cnn'] =news_dump_predicted['label_cnn'].replace(replaced_label_finbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dump_predicted.drop(columns=['proba_cnn'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dump_predicted.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "path_data = '/mnt/files/workdata/work/python-scripts/prediction_analyzer/predict-stock-quotes/data/' \n",
    "file_path = path_data + 'news_dump_predicted.csv'\n",
    "# news_dump_predicted.to_csv(file_path, index=False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare results models\n",
    "print(classification_report(news_dump_predicted['label_sgd'].astype(int), news_dump_predicted['label_cnn'] , digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '/mnt/files/workdata/work/python-scripts/prediction_analyzer/predict-stock-quotes/data/' \n",
    "file_path = path_data + 'news_dump_predicted.csv'\n",
    "news_dump_predicted = pd.read_csv(file_path,  dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dump_predicted.text2 = np.where(news_dump_predicted.text2.isnull(), ' ', news_dump_predicted.text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dump_predicted.text2 = news_dump_predicted.text2.apply(lambda x: x[:500])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = '\\ttext\\tlabel'\n",
    "news_dump_predicted[col_name] = news_dump_predicted.index.astype('str') + '\\t' + news_dump_predicted.text2 + '\\t' + 'neutral'\n",
    "news_dump_predicted[col_name] = news_dump_predicted[col_name].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "path_data_test = '/mnt/files/workdata/work/python-scripts/prediction_analyzer/predict-stock-quotes/data/' \n",
    "file_path = path_data_test + 'test_news_for_finbert.csv'\n",
    "news_dump_predicted[[col_name]].to_csv(file_path,index=False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_best_sentiment_model = 'keras_cnn_' + research_sentiment+ '_model'\n",
    "# model.save('models/tmp/' + name_best_sentiment_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_best_sentiment_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model2 = load_model('models/tmp/' + name_best_sentiment_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba = model2.predict(x_test_seq)\n",
    "test['predict_proba'] = predict_proba.tolist()\n",
    "test['predict_label'] = test.predict_proba.apply(lambda x: np.argmax(x,axis=0))\n",
    "print(classification_report(y_test, test['predict_label'] , digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "print('Research sentiment {}'.format(research_sentiment))\n",
    "\n",
    "# threshhold = 50\n",
    "# threshhold = 100 - Counter(train['label'])[1] / len( train['label']) * 100\n",
    "predict_proba = model.predict(x_test_seq)\n",
    "\n",
    "# predicted = np.where(predict_proba > np.percentile(predict_proba, threshhold), 1, 0)\n",
    "print(classification_report(y_test, predicted, digits=5))\n",
    "\n",
    "mcc = matthews_corrcoef(y_test, predicted)\n",
    "print('matthews_corrcoef =  {:04.4f}'.format(mcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_check(count_segmentations=5):\n",
    "    evaluate_results = []\n",
    "    for _ in range(count_segmentations):\n",
    "        tweet_input = Input(shape=(SENTENCE_LENGTH,), dtype='int32')\n",
    "        tweet_encoder = Embedding(NUM, DIM, input_length=SENTENCE_LENGTH, weights=[embedding_matrix], trainable=False)(tweet_input)\n",
    "\n",
    "        branches = []\n",
    "        x = Dropout(0.2)(tweet_encoder)\n",
    "\n",
    "        for size, filters_count in [(2, 10), (3, 10), (4, 10), (5, 10), (6,10), (7,10)]: \n",
    "            for i in range(filters_count):\n",
    "                # Добавляем слой свертки\n",
    "                branch = Conv1D(filters=1, kernel_size=size, padding='valid', activation='relu')(x)\n",
    "                branch = GlobalMaxPooling1D()(branch)\n",
    "                branches.append(branch)\n",
    "\n",
    "        x = concatenate(branches, axis=1)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = Dense(30, activation='relu')(x)\n",
    "        x = Dense(last_dence)(x)\n",
    "        output = Activation(last_activation)(x) \n",
    "\n",
    "        model = Model(inputs=[tweet_input], outputs=[output])\n",
    "        model.compile(loss=loss_func, optimizer='adam', metrics=[matthews_correlation])\n",
    "        model._name = research_sentiment\n",
    "\n",
    "        sample_weight = np.ones(shape=(len(y_train),))\n",
    "        sample_weight[y_train == 1] = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "\n",
    "        checkpoint = ModelCheckpoint(\"models/cnn/cnn-frozen-embed-{}\".format(model.name) + \"-{epoch:02d}-{val_matthews_correlation:.4f}.hdf5\",\n",
    "                                     monitor='val_matthews_correlation', save_best_only=True, mode='max', period=1)\n",
    "\n",
    "#         history = model.fit(x_train_seq, y_train, batch_size=32, epochs=epochs1, validation_split=0.25, sample_weight=sample_weight, \n",
    "#                             callbacks = [checkpoint], verbose=0)\n",
    "        history = model.fit(x_train_seq, y_train, batch_size=32, epochs=epochs1, validation_split=0.25, class_weight=class_weight, \n",
    "                            callbacks = [checkpoint], verbose=0)\n",
    "\n",
    "        model.layers[1].trainable = True\n",
    "        adam = optimizers.Adam(lr=0.0001)\n",
    "        model.compile(loss=loss_func, optimizer=adam, metrics=[matthews_correlation])\n",
    "\n",
    "        checkpoint = ModelCheckpoint(\"models/cnn/loop-{}-cnn-frozen-embed-{}\".format(i, model.name) + \"-{epoch:02d}-{val_matthews_correlation:.4f}.hdf5\", \n",
    "                                     monitor='val_matthews_correlation', save_best_only=True, mode='max', period=1)\n",
    "\n",
    "#         history = model.fit(x_train_seq, y_train, batch_size=32, epochs=epochs2, validation_split=0.25, shuffle=True, callbacks = [checkpoint], verbose=0) # sample_weight=sample_weight,\n",
    "        \n",
    "        history = model.fit(x_train_seq, y_train, batch_size=32, epochs=epochs1, validation_split=0.25, class_weight=class_weight, shuffle=True, \n",
    "                            callbacks = [checkpoint], verbose=0)\n",
    "        \n",
    "        result = model.evaluate(x_test_seq, y_test, verbose=0)\n",
    "        evaluate_results.append(result[1])\n",
    "    \n",
    "    mean_result = sum(evaluate_results)/len(evaluate_results)\n",
    "    \n",
    "    print('==================================================')\n",
    "    print('mean_matthews_correlation = {}'.format(mean_result))\n",
    "        \n",
    "    return mean_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_results = cross_val_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_results - 0.701 epoch2 - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "print('Research sentiment {}'.format(research_sentiment))\n",
    "\n",
    "# threshhold = 50\n",
    "threshhold = 100 - Counter(train['label'])[1] / len( train['label']) * 100\n",
    "predict_proba = model.predict(x_test_seq)\n",
    "\n",
    "predicted = np.where(predict_proba > np.percentile(predict_proba, threshhold), 1, 0)\n",
    "print(classification_report(y_test, predicted, digits=5))\n",
    "\n",
    "mcc = matthews_corrcoef(y_test, predicted)\n",
    "print('matthews_corrcoef =  {:04.4f}'.format(mcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'pred': predict_proba[:,0], 'label': y_test.numpy()}\n",
    "df_data = pd.DataFrame(data)\n",
    "df_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_best_sentiment_model = 'keras_cnn_' + model.name + '_model.hdf5'\n",
    "model.save('models/' + name_best_sentiment_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Primary model, matthews_corrcoef = 0.5310 with positive label\n",
    "##### Primary model, matthews_corrcoef = 0.2746 with negative label\n",
    "##### Primary model, matthews_corrcoef = 0.5801 with neutral label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('models/cnn/' + name_best_sentiment_model, custom_objects={'precision': precision, 'recall':recall, 'f1':f1, 'matthews_correlation':matthews_correlation})\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[precision, recall, f1, matthews_correlation]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba = model.predict(x_test_seq)\n",
    "test['predict_proba'] = predict_proba.tolist()\n",
    "test['predict_label'] = test.predict_proba.apply(lambda x: np.argmax(x,axis=0))\n",
    "print(classification_report(y_test, test['predict_label'] , digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshhold = 100 - Counter(train['label'])[1] / len( train['label']) * 100\n",
    "# predicted = model.predict(x_test_seq)\n",
    "\n",
    "# predicted = np.where(predicted > np.percentile(predicted, threshhold) , 1, 0)\n",
    "# print(classification_report(y_test, predicted, digits=5))\n",
    "\n",
    "# print(Counter(predicted[:,0]), '\\n')\n",
    "# print('matthews_corrcoef =  {:04.4f}'.format(matthews_corrcoef(y_test, predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras set up hyperparameters - talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparametrs research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talos\n",
    "from talos.utils import hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tuning\n",
    "# Заморозить все слои предварительно обученной модели. Добавить свои слои к обученной модели. \n",
    "# Обучить добавленные слои. Разморозить несколько верхних слоев. Обучить эти слои и добавленную часть вместе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_model(x_train, y_train, params, checkpoint): \n",
    "       \n",
    "    branches = []\n",
    "    x = Dropout(params['dropout'][0])(tweet_encoder)\n",
    "\n",
    "    for size, filters_count in [(2, 10), (3, 10), (4, 10), (5, 10), (6,10), (7,10)]: \n",
    "        for i in range(filters_count):\n",
    "            branch = Conv1D(filters=1, kernel_size=size, padding='valid', activation='relu')(x)\n",
    "            branch = GlobalMaxPooling1D()(branch)\n",
    "            branches.append(branch)\n",
    "\n",
    "    x = concatenate(branches, axis=1)\n",
    "    x = Dropout(params['dropout'][0])(x)\n",
    "    x = Dense(params['first_neuron'][0], activation=params['activation'][0])(x)\n",
    "    x = Dense(1)(x)\n",
    "    \n",
    "    output = Activation(params['last_activation'][0])(x)\n",
    "\n",
    "    model = Model(inputs=[tweet_input], outputs=[output])\n",
    "    \n",
    "    model.compile(loss=params['losses'][0], optimizer=params['optimizer'][1], metrics=[matthews_correlation])\n",
    "    \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_split=0.25,\n",
    "                        batch_size=params['batch_size'][1],\n",
    "                        epochs=params['epochs'][0],\n",
    "                        verbose=1,\n",
    "                        callbacks = [checkpoint]\n",
    "                       )\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "     'lr': [0.0001],    \n",
    "     'activation':['relu', 'elu'],\n",
    "     'optimizer': ['Nadam', 'Adam'],   \n",
    "     'losses': ['binary_crossentropy', 'logcosh'],\n",
    "     'shapes': ['brick', 'long_funnel'], # <<< required\n",
    "     'first_neuron': [32, 64],     # <<< required\n",
    "     'hidden_layers':[1, 2, 3],    # <<< required\n",
    "     'dropout': [.2, .3],          # <<< required\n",
    "     'batch_size': [20, 30, 40],\n",
    "     'epochs': (10, 40, 10),\n",
    "     'last_activation': ['sigmoid']\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = {'lr': (0.8, 1.2, 3),\n",
    "#      'first_neuron':[4, 8, 16, 32, 64],\n",
    "#      'hidden_layers':[0, 1, 2],\n",
    "#      'batch_size': (1, 5, 5),\n",
    "#      'epochs': [50, 100, 150],\n",
    "#      'dropout': (0, 0.2, 3),\n",
    "#      'weight_regulizer':[None],\n",
    "#      'emb_output_dims': [None],\n",
    "#      'shape':['brick','long_funnel'],\n",
    "#      'kernel_initializer': ['uniform','normal'],\n",
    "#      'optimizer': [Adam, Nadam, RMSprop],\n",
    "#      'losses': [binary_crossentropy],\n",
    "#      'activation':[relu, elu],\n",
    "#      'last_activation': [sigmoid]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talos_checkpoint = ModelCheckpoint(\"models/cnn/for-talos-cnn-frozen-embed-{}\".format(research_sentiment) + \"-{epoch:02d}-{val_matthews_correlation:.4f}.hdf5\",\n",
    "                             monitor='val_matthews_correlation', save_best_only=True, mode='max', period=1)\n",
    "\n",
    "history_talos = sentiment_model(x_train_seq, y_train, p, talos_checkpoint) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = history_talos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshhold = 100 - Counter(train['label'])[1] / len( train['label']) * 100\n",
    "predicted = model.predict(x_test_seq)\n",
    "\n",
    "predicted = np.where(predicted > np.percentile(predicted, threshhold) , 1, 0)\n",
    "print(classification_report(y_test, predicted, digits=5))\n",
    "\n",
    "print(Counter(predicted[:,0]), '\\n')\n",
    "print('matthews_corrcoef =  {:04.4f}'.format(matthews_corrcoef(y_test, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dropout(params['dropout'][0])(tweet_encoder)\n",
    "\n",
    "    x = concatenate(branches, axis=1)\n",
    "    x = Dropout(params['dropout'][0])(x)\n",
    "    x = Dense(params['first_neuron'][0], activation=params['activation'][0])(x)\n",
    "    x = Dense(1)(x)\n",
    "    \n",
    "    output = Activation(params['last_activation'][0])(x)\n",
    "\n",
    "    model = Model(inputs=[tweet_input], outputs=[output])\n",
    "    \n",
    "    model.compile(loss=params['losses'][0], optimizer=params['optimizer'][1], metrics=[matthews_correlation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reserch_model_hyperparams(x_train, y_train, x_val, y_val, params, model=model):\n",
    "\n",
    "    model.layers[1].trainable = True\n",
    "\n",
    "    \n",
    "    hidden_layers(model, params, 1)\n",
    "\n",
    "    adam = optimizers.Adam(lr=params['lr'])\n",
    "\n",
    "    model.compile(loss=params['losses'], optimizer=params['optimizer'], metrics=[matthews_correlation])\n",
    "    \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_split=0.25,\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0,\n",
    "                        )\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in  range(10):\n",
    "#     checkpoint = ModelCheckpoint(\"models/cnn/talos_best_model-{}\".format(i, model.name) + \"-{epoch:02d}-{val_matthews_correlation:.4f}.hdf5\", \n",
    "#                                  monitor='val_matthews_correlation', save_best_only=True, mode='max', period=1)\n",
    "#     history = model.fit(x_train_seq, y_train, batch_size=, epochs=EPOCHS, validation_split=0.25, callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ta_params = dict()  \n",
    "#  'kernel_initializer': ['uniform','normal']}\n",
    "#   optimizer - 'ftrl' лучше всего работает с разреженными данными, такими как ввод языка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talos_scan = talos.Scan(x=x_train_seq, y=y_train.numpy(), model=reserch_model_hyperparams, params=p, \n",
    "                          reduction_metric='val_matthews_correlation', experiment_name='neutral_sentiment', \n",
    "                          fraction_limit=0.05, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talos_scan.data.sort_values(by='val_matthews_correlation', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talos_scan.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talos_scan.evaluate_models(x_val=x_train_seq,\n",
    "                            y_val=y_train.numpy(),\n",
    "                            n_models=10,\n",
    "                            metric='matthews_correlation',\n",
    "                            folds=10,\n",
    "                            shuffle=True,\n",
    "                            task='binary',\n",
    "                            asc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talos_scan.data.sort_values(by='eval_f1score_std', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talos_best_model = talos_scan.best_model(metric='eval_f1score_std', asc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshhold = 100 - Counter(train['label'])[1] / len( train['label']) * 100\n",
    "predicted = talos_best_model.predict(x_test_seq)\n",
    "\n",
    "predicted = np.where(predicted > np.percentile(predicted, threshhold) , 1, 0)\n",
    "print(classification_report(y_test, predicted, digits=5))\n",
    "\n",
    "print(Counter(predicted[:,0]), '\\n')\n",
    "print('matthews_corrcoef =  {:04.4f}'.format(matthews_corrcoef(y_test, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results distribution\n",
    "sns.jointplot(x=\"val_acc\", y=\"val_categorical_crossentropy\", data=df_results);\n",
    "sns.jointplot(x=\"categorical_crossentropy\", y=\"val_categorical_crossentropy\", data=df_results);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
